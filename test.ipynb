{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a4da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents(urls, save_folder='documents'):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    for url in urls:\n",
    "        # Check and handle the Microsoft slide URL explicitly\n",
    "        if 'microsoft.com' in url and 'SlidesFY25Q2' in url:\n",
    "            filename = 'SlidesFY25Q2.pptx'\n",
    "        else:\n",
    "            filename = url.split('/')[-1].split('?')[0]\n",
    "            if not filename.endswith('.pdf') and not filename.endswith('.pptx'):\n",
    "                filename += '.pdf'  # default extension if unclear\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            filepath = os.path.join(save_folder, filename)\n",
    "            with open(filepath, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f'Downloaded: {filename}')\n",
    "        else:\n",
    "            print(f'Failed to download {url} (status code: {response.status_code})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3736da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: SlidesFY25Q2.pptx\n",
      "Downloaded: TSLA-Q4-2024-Update.pdf\n",
      "Downloaded: 10Q-Q1-2025-as-filed.pdf\n",
      "Downloaded: FY25_Q1_Consolidated_Financial_Statements.pdf\n",
      "Downloaded: _10-K-2021-(As-Filed).pdf\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "urls = [\n",
    "    \"https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/SlidesFY25Q2\",\n",
    "    \"https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q4-2024-Update.pdf\",\n",
    "    \"https://s2.q4cdn.com/470004039/files/doc_earnings/2025/q1/filing/10Q-Q1-2025-as-filed.pdf\",\n",
    "    \"https://www.apple.com/newsroom/pdfs/fy2025-q1/FY25_Q1_Consolidated_Financial_Statements.pdf\",\n",
    "    \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\"\n",
    "]\n",
    "\n",
    "fetch_documents(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537128c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined categories \n",
    "CATEGORIES = {\n",
    "    \"Financial Reports\": [\"annual report\", \"quarterly report\", \"earnings\"],\n",
    "    \"Investor Presentations\": [\"presentation\", \"conference\", \"slides\"],\n",
    "    \"Corporate Governance Documents\": [\"policy\", \"charter\", \"governance\"],\n",
    "    \"Press Releases\": [\"announcement\", \"merger\", \"leadership\"],\n",
    "    \"Stock Market Information\": [\"stock price\", \"dividend\", \"shareholder\"],\n",
    "    \"Corporate Social Responsibility (CSR) Reports\": [\"sustainability\", \"ESG\", \"community\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb1699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elements(filepath):\n",
    "    if filepath.endswith('.pdf'):\n",
    "        elements = partition_pdf(filename=filepath,skip_infer_table_types=False, strategy='hi_res')\n",
    "    elif filepath.endswith('.pptx'):\n",
    "        elements = partition_pptx(filename=filepath)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}\")\n",
    "    return elements\n",
    "\n",
    "def classify_document(text):\n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        if any(keyword.lower() in text.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "def save_extracted_data(filepath, elements, output_folder=\"extracted_data\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Convert elements to dictionaries\n",
    "    elements_dict = [element.to_dict() for element in elements]\n",
    "    filename = os.path.basename(filepath).rsplit('.', 1)[0] + '.json'\n",
    "\n",
    "    with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as file:\n",
    "        json.dump(elements_dict, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93698a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FY25_Q1_Consolidated_Financial_Statements.pdf: Extracted and classified as Financial Reports\n",
      "SlidesFY25Q2.pptx: Extracted and classified as Unknown\n",
      "TSLA-Q4-2024-Update.pdf: Extracted and classified as Financial Reports\n",
      "_10-K-2021-(As-Filed).pdf: Extracted and classified as Financial Reports\n",
      "10Q-Q1-2025-as-filed.pdf: Extracted and classified as Financial Reports\n"
     ]
    }
   ],
   "source": [
    "def process_documents(folder='documents'):\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if os.path.isfile(filepath) and filepath.endswith(('.pdf', '.pptx')):\n",
    "            try:\n",
    "                elements = extract_elements(filepath)\n",
    "                full_text = \" \".join([element.text for element in elements if hasattr(element, 'text')])\n",
    "                category = classify_document(full_text)\n",
    "                save_extracted_data(filepath, elements)\n",
    "                print(f\"{filename}: Extracted and classified as {category}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [el for el in elements if hasattr(el, 'table') and el.table]\n",
    "for table in tables:\n",
    "    print(f\"Table found in {filepath}:\")\n",
    "    print(table.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb64a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id ee9df6e9-6181-4872-ad1f-c0512b5cb7b2\n",
      "Successfully processed and saved: FY25_Q1_Consolidated_Financial_Statements.pdf\n",
      "Started parsing the file under job_id b377af2b-cb01-48cc-9966-551389e856e7\n",
      "Successfully processed and saved: SlidesFY25Q2.pptx\n",
      "Started parsing the file under job_id 51f0f918-e2ae-46e6-8d91-eabca3fc08c3\n",
      "Successfully processed and saved: TSLA-Q4-2024-Update.pdf\n",
      "Started parsing the file under job_id 0b042626-99ef-412b-8b5d-6047f24ca841\n",
      "Successfully processed and saved: _10-K-2021-(As-Filed).pdf\n",
      "Started parsing the file under job_id 1bb35f65-02d6-4bb2-9bba-ab2926a2dca2\n",
      "Successfully processed and saved: 10Q-Q1-2025-as-filed.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from llama_cloud_services import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables and setup\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "parser = LlamaParse(result_type=\"markdown\")\n",
    "\n",
    "documents_folder = 'documents'\n",
    "output_jsonl = 'parsed_documents.jsonl'\n",
    "\n",
    "# Define document categories based on keywords\n",
    "CATEGORIES = {\n",
    "    \"Financial Reports\": [\"annual report\", \"quarterly report\", \"earnings\"],\n",
    "    \"Investor Presentations\": [\"presentation\", \"conference\", \"slides\"],\n",
    "    \"Corporate Governance Documents\": [\"policy\", \"charter\", \"governance\"],\n",
    "    \"Press Releases\": [\"announcement\", \"merger\", \"leadership\"],\n",
    "    \"Stock Market Information\": [\"stock price\", \"dividend\", \"shareholder\"],\n",
    "    \"Corporate Social Responsibility (CSR) Reports\": [\"sustainability\", \"ESG\", \"community\"]\n",
    "}\n",
    "\n",
    "# Function to classify documents based on keywords\n",
    "def classify_document(text):\n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        if any(keyword.lower() in text.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Initialize LangChain splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Parse documents, chunk content, and write structured output\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for file in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file)\n",
    "        if file.lower().endswith(('.pdf', '.pptx')):\n",
    "            doc_type = 'pdf' if file.lower().endswith('.pdf') else 'ppt'\n",
    "            try:\n",
    "                llama_docs = parser.load_data(file_path)\n",
    "                for doc in llama_docs:\n",
    "                    content = doc.text\n",
    "                    page_num = doc.metadata.get('page', 0)\n",
    "                    source = file\n",
    "                    chunks = text_splitter.split_text(content)\n",
    "\n",
    "                    for chunk in chunks:\n",
    "                        chunk_id = str(uuid.uuid4())\n",
    "                        content_type = 'table' if '|' in chunk else 'text'\n",
    "                        category = classify_document(chunk)\n",
    "\n",
    "                        json_obj = {\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"content\": chunk,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": source,\n",
    "                                \"page_num\": page_num,\n",
    "                                \"doc_type\": doc_type,\n",
    "                                \"content_type\": content_type,\n",
    "                                \"category\": category\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        jsonl_file.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')\n",
    "                print(f\"Successfully processed and saved: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39874371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id dc8653a1-a131-4e68-8f35-d953363e0858\n",
      "Successfully processed and saved: FY25_Q1_Consolidated_Financial_Statements.pdf\n",
      "Started parsing the file under job_id 1e1942e7-4f8b-430d-8a47-d5295d0b54e8\n",
      "Error while parsing the file 'documents/SlidesFY25Q2.pptx': Server disconnected without sending a response.\n",
      "Successfully processed and saved: SlidesFY25Q2.pptx\n",
      "Started parsing the file under job_id 940f500f-3001-4db2-8628-71a100223a75\n",
      "Error while parsing the file 'documents/TSLA-Q4-2024-Update.pdf': Server disconnected without sending a response.\n",
      "Successfully processed and saved: TSLA-Q4-2024-Update.pdf\n",
      "Started parsing the file under job_id 388ad148-b11a-4f61-812f-5264b9b1527c\n",
      "Successfully processed and saved: _10-K-2021-(As-Filed).pdf\n",
      "Started parsing the file under job_id 84dc2661-a273-4535-b932-124b83705390\n",
      "Successfully processed and saved: 10Q-Q1-2025-as-filed.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from llama_cloud_services import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables and setup\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "# Initialize the parser (markdown or text)\n",
    "parser = LlamaParse(result_type=\"markdown\")\n",
    "\n",
    "documents_folder = 'documents'\n",
    "output_jsonl = 'parsed_documents.jsonl'\n",
    "\n",
    "# Define document categories based on keywords\n",
    "CATEGORIES = {\n",
    "    \"Financial Reports\": [\"annual report\", \"quarterly report\", \"earnings\"],\n",
    "    \"Investor Presentations\": [\"presentation\", \"conference\", \"slides\"],\n",
    "    \"Corporate Governance Documents\": [\"policy\", \"charter\", \"governance\"],\n",
    "    \"Press Releases\": [\"announcement\", \"merger\", \"leadership\"],\n",
    "    \"Stock Market Information\": [\"stock price\", \"dividend\", \"shareholder\"],\n",
    "    \"Corporate Social Responsibility (CSR) Reports\": [\"sustainability\", \"ESG\", \"community\"]\n",
    "}\n",
    "\n",
    "# Function to classify documents based on keywords\n",
    "def classify_document(text):\n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        if any(keyword.lower() in text.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Initialize LangChain splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Parse documents, chunk content, and write structured output\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for file in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file)\n",
    "        if file.lower().endswith(('.pdf', '.pptx')):\n",
    "            doc_type = 'pdf' if file.lower().endswith('.pdf') else 'ppt'\n",
    "            try:\n",
    "                llama_docs = parser.load_data(file_path)\n",
    "                for doc in llama_docs:\n",
    "                    content = doc.text\n",
    "                    page_num = doc.metadata.get('page', 0)\n",
    "                    source = file\n",
    "                    chunks = text_splitter.split_text(content)\n",
    "\n",
    "                    for chunk in chunks:\n",
    "                        chunk_id = str(uuid.uuid4())\n",
    "                        content_type = 'table' if '|' in chunk else 'text'\n",
    "                        category = classify_document(chunk)\n",
    "\n",
    "                        json_obj = {\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"content\": chunk,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": source,\n",
    "                                \"page_num\": page_num,\n",
    "                                \"doc_type\": doc_type,\n",
    "                                \"content_type\": content_type,\n",
    "                                \"category\": category\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        jsonl_file.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')\n",
    "                print(f\"Successfully processed and saved: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_masterdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
